import numpy as np
import pandas as pd
def logisticRegression_GD(X,Y,iter=1000,lr=0.01):
    coef=[]
    result=[]
    X=pd.DataFrame(X)
    #for i in range(0,len(list(pd.DataFrame(X)))+1):
        #coef.append(0)
    coef=np.zeros(len(list(pd.DataFrame(X)))+1)
    #coef=[0.27371834,-0.70590352, -0.15846683,  0.26224]
    for i in range(iter):
        counter=1
        hypo=0
        ##### Determine hypo ####
        while counter <= len(coef):
            if counter-1 == 0:
                hypo = coef[counter-1]
            else:
                hypo=hypo+coef[counter-1]*X.iloc[:,counter-2]
            counter=counter+1
        shypo=1/(1+np.exp(-hypo))
        error_1=(shypo-Y)
        #print(error_1)
        #########################
        counter=1
        der=[]
        newtheta=[]
        ############ Gradient descent #########
        while counter <= len(coef):
            if counter-1 == 0:
                error=(error_1*1).sum()
            else:
                error=(error_1*list(X.iloc[:,counter-2])).sum()
            der.append(error)
            newtheta.append(round(coef[counter-1] - ((lr/len(Y))*error),5))
            counter=counter+1
        ####### Cost #######
        cost=round((-1/(len(Y)))*((Y*np.log(shypo)+(1-Y)*np.log(1-shypo)).sum()),10)
                   #(error_1**2).sum(),10)
        ####################
        coef=newtheta[:]
        result.append([coef,cost])
        if len(result) >=2 and result[-1][1] > result[-2][1]:
            print("Check the learning rate.Gradient descent is increasing instead of decreasing")
            if len(result) == 2:
                print(result)
            else:
                print(result[-5:])
                break
        elif len(result) >=2 and result[-1][1] == result[-2][1]:
            break
    if len(result) >=2 and round(result[-1][1],5) < round(result[-2][1],5):
        print("We haven't reached the local minima yet. Try changing iterations or learning rate")
        
    return(result[-1])
